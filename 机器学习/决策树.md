# 决策树算法

决策树算法 (Decision Tree algorithms)

* ID3算法基本思路：
> * 首先递归法构建决策树```createDecisionTree(dataSet, labels)```
>   * 对于给定的数据集，通过计算不同特征划分数据时的香农熵，选取最佳分类特征（同时```labels```中出去该特征）
>   * 以最佳特征作为该树的根节点，遍历该特征的所有可能值，来构建子树
>   * 构建子树时，递归调用函数```createDecisionTree(splitDataSet(dataSet, bestFeat, value), subLabels)```，递归的结束条件：
>       * 类别完全相同则停止继续划分，返回该类标签
>       * 使用完了所有的特征，仍然不能将数据集划分成仅包含该类的分组，则返回出现次数最多的类别
> * 使用决策树分类```classify(tree, featLabels, testVector)```：
>   * 确定第一个用于划分数据集的特征，即决策树的根节点对应特征
>   * 确定该特征在数据集中的位置
>   * 遍历整棵树，比较```testVector```中的值与树节点的值，如果到达叶子节点，则返回当前节点的分类标签

## ID3算法产生决策树：
```python
# -*- coding: utf-8 -*-
from numpy import *
import operator

# 计算特定数据集的香农熵
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    # 为所有可能分类创建dict
    for featVector in dataSet: # 遍历样本集
        currentLabel1 = featVector[-1] # 每个样本的类
        if currentLabel1 not in labelCounts.keys():
            labelCounts[currentLabel1] = 0
        labelCounts[currentLabel1] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key]) / numEntries # 概率
        shannonEnt -= prob * log2(prob) # p * log2p 求和，即香农熵
    return shannonEnt

# 按照给定特征划分数据集
# 返回 dataSet 中第 axis 维的值等于 value 的样本集
def splitDataSet(dataSet, axis, value):
    # 创建新的 list
    retDataSet = []
    # 抽取 dataSet 中第 axis 维的值等于 value 的样本
    for featVector in dataSet:
        if featVector[axis] == value:
            # 去除掉 featVector[axis]项
            reducedFeatVector = featVector[:axis]
            reducedFeatVector.extend(featVector[axis+1:])
            retDataSet.append(reducedFeatVector)
    return retDataSet

# 选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1
    baseEntropy = calcShannonEnt(dataSet) # 数据集的香农熵
    bestInfoGain = 0.0 # 最佳信息熵增益
    bestFeature = -1 # 最佳信息熵对应的特征
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList) # 第i维的所有可能值
        newEntropy = 0.0
        for value in uniqueVals: # 遍历第i维的所有可能值
            subDataSet = splitDataSet(dataSet, i, value) # 以第i维为特征划分得到子集
            prob = len(subDataSet) / float(len(dataSet))
            newEntropy -= prob * log2(prob) # 以第i维为特征划分的信息熵
        infoGain = baseEntropy - newEntropy # 信息熵增益
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain # 最佳信息熵增益
            bestFeature = i # 最佳信息熵对应的特征
    return bestFeature

# 所有特征都用完却仍然还不能够分类完全时，采用多数表决策略
def majorityCnt(classList):
    classCount = []
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
        classCount[vote] += 1
        sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
        return sortedClassCount[0][0]

# 递归构建决策树
def createDecisionTree(dataSet, labels):
    classList = [example[-1] for example in dataSet]
    # 递归函数的第一个停止条件：
    # 类别完全相同则停止继续划分，返回该类标签
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 递归函数的第二个停止条件：
    # 使用完了所有的特征，仍然不能将数据集划分成仅包含该类的分组，则返回出现次数最多的类别
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)

    bestFeat = chooseBestFeatureToSplit(dataSet) # 最佳分类特征
    bestFeatLabel = labels[bestFeat] # 最佳分类特征对应的标签
    tree = { bestFeatLabel: {} } # 最佳分类特征作为父节点
    del(labels[bestFeat]) # 删除最佳特征对应的标签
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues) # 最佳特征对应的所有可能值
    for value in uniqueVals:
        subLabels = labels[:]
        # 子树
        tree[bestFeatLabel][value] = createDecisionTree(splitDataSet(dataSet, bestFeat, value), subLabels)
    return tree

# 使用决策树的分类函数
def classify(tree, featLabels, testVector):
    firstStr = list(tree.keys())[0] # tree的根节点对应的分类
    secondDict = tree[firstStr] # 子节点dict对象
    featIndex = featLabels.index(firstStr)
    for key in secondDict.keys():
        if testVector[featIndex] == key:
            if type(secondDict[key]).__name__ == 'dict':
                classLabel = classify(secondDict[key], featLabels, testVector)
            else:
                classLabel = secondDict[key]
    return classLabel
```

《机器学习·实战》中的测试：
```python
# -*- coding: utf-8 -*-
from numpy import *
import operator
import DT


# ===================test1===================
def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    labels = ['no surfacing','flippers']
    return dataSet, labels

dataSet, labels = createDataSet()
tree = DT.createDecisionTree(dataSet, labels)
print(tree)
# {'flippers': {0: 'no', 1: {'no surfacing': {'yes': 'yes', 'no': 'no'}}}}
print(DT.classify(tree, ['no surfacing','flippers'], [1, 1]))


# ===================test2===================
# 绘制test1中的决策树
import matplotlib.pyplot as plt

# 定义文本框和箭头格式
decisionNode = dict(boxstyle="sawtooth", fc="0.8")
leafNode = dict(boxstyle="round4", fc="0.8")
arrow_args = dict(arrowstyle="<-")

# 获取叶节点的数目
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            numLeafs += getNumLeafs(secondDict[key])
        else:
            numLeafs += 1
    return numLeafs

# 获取树的层数
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            thisDepth = 1 + getTreeDepth(secondDict[key]) # 注意和计算叶子节点数目的不同
        else:
            thisDepth = 1
        # 取各个子树最深的层数
        if thisDepth > maxDepth:
            maxDepth = thisDepth
    return maxDepth

# 绘制带箭头的注解
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',
                            xytext=centerPt, textcoords='axes fraction',
                            va="center", ha="center", bbox=nodeType, arrowprops=arrow_args)

# 在父子节点之间填充文本信息
def plotMidText(cntrPt, parentPt, txtString):
    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]
    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)

def plotTree(myTree, parentPt, nodeTxt):
    numLeafs = getNumLeafs(myTree)  # 叶子节点个数，决定x坐标有多宽
    depth = getTreeDepth(myTree) # 树的最大深度，决定y坐标有多宽
    firstStr = list(myTree.keys())[0]  # 该节点的标签
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff)
    plotMidText(cntrPt, parentPt, nodeTxt)
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict': # 如果叶子节点还有子节点，则递归绘制子树
            plotTree(secondDict[key], cntrPt, str(key))
        else:
            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD

def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)  # no ticks
    # createPlot.ax1 = plt.subplot(111, frameon=False) # ticks for demo puropses
    plotTree.totalW = float(getNumLeafs(inTree)) # 树的宽度，即叶子节点的数目
    plotTree.totalD = float(getTreeDepth(inTree)) # 树的深度
    plotTree.xOff = -0.5 / plotTree.totalW # x轴方向的间隔
    plotTree.yOff = 1.0 # y轴方向的间隔
    plotTree(inTree, (0.5, 1.0), '')
    plt.show()

createPlot(tree)


# ===================test3===================
# 通过pickle存储和读取决策树
import pickle

def storeTree(inputTree, filename):
    fw = open(filename, 'w')
    pickle.dump(inputTree, fw)
    fw.close()

def grabTree(filename):
    fr = open(filename)
    return pickle.load(fr)

fr = open('./datas/lenses.txt')
lenses = [inst.strip().split('\t') for inst in fr.readlines()]
lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']
tree = DT.createDecisionTree(lenses, lensesLabels)
print(tree)
createPlot(tree)
```