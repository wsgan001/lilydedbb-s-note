# 《数学之美（吴军）》笔记

### 搜索引擎质量的度量

1. 完备的索引
2. 对网页质量的度量（如 PageRank 算法）
3. 用户偏好
4. 确定一个网页和某个查询的相关性的方法（TF-IDF）

### TF-IDF (Term Frenquency / Inverse Document Frequency)

`TF` (Term Frenquency) 为文本词频，即文本中某字词的出现次数；`IDF` (Inverse Document Frequency) 为逆文本频率指数

`IDF` 公式为

```
IDF = log(D / D_w)
```

`D` 为全部网页（文本）数，`D_w` 为出现关键词 `w` 的网页（文本）数

`D_w` 越大，`w` 的权重越小；反之亦然

```
相关性 = TF_1 * IDF_1 + TF_2 * IDF_2 + ... + TF_n * IDF_n
```

### 有限状态机（地图应用）和动态规划（全球导航）

待更新...

### 余弦定理在分类中的应用

特征向量（feature vector）：

对于一篇文本中的所有实词，计算出他们的 `TF-IDF` 值

向量间的距离 ———— 余弦定理：

```
cos(A) = <b, c> / (|b| * |c|)
```

假设文本 `X` 和 `Y` 的特征向量分别为：`x1, x2, ...., xn` 和 `y1, y2, ..., yn`

```
cos(theta) = (x1 * y1 + ... + xn * yn) / (sqrt(x1^2 + ... + xn^2) * sqrt(y1^2 + ... + yn^2))
```

分类：

1. 计算所有文本的特征向量两两之间的余弦相似性，把相似性大于一个阈值的文本合并为一个小类（subclass）
2. 把每个小类中所有的新闻作为一个整体，计算小类的特征向量，再计算小类之间两两的余弦相似性，然后合并成大一些的小类；不断重复下去，直到某一类中的一些文本之间的相似性很小了，停止迭代过程

**技巧：大数据量时的余弦计算**

（真实的文本分类场景，词汇表的大小（即向量的长度）都是几十万几百万，计算向量的长度和内积会比较耗时；尤其需要分类的文本数量非常大时，比如几百万个文本需要分类，计算量可达10的十几次方数量级）

1. 余弦公式的分母部分不需要重复计算，每计算一个新的向量的长度之后，就将其长度存起来，之后再需要时，直接使用即可
2. 计算分子中两个向量的内积使，只考虑非零元素，可以是计算复杂度下降大约100倍
3. 删除虚词，诸如：“因为”、“所以”、“非常”、“之”、“乎”、“者”、“也”、“和”等等（这样不仅可以提高计算速度，还可以减少噪声，虚词的权重实际上就是一种噪声）

**技巧：位置的加权