# 集成学习 (Ensemble Learning)

## Boosting

先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个学习器；如此重复进行，直至基学习器数目达到事先制定的值T，最终将这T个基学习器进行加权结合

AdaBoost 伪代码：

```
输入：训练集 D = {(x1, y1), (x2, y2), ... , (xm, my)};
     基学习算法 L;
     训练轮数 T;
过程：
D_1(x) = 1 / m; // 初始化样本全职分布
for t = 1, 2, ... , T do
h_t = L(D, D_t); // 基于分布 Dt 从数据集 D 中训练出分类器 ht
ct = P(h_t(x) != f(x)); // 估计 ht 的误差
if ct > 0.5 then break;
a_t = (1 / 2) ln[(1 - ct) / ct]; // 确定分类器 ht 的权重
D_t+1(x) = (D_t(x) / Z_t * [h_t(x) == f(x)] ? exp(-a_t) : exp(at))
end for
输出：H(x) = sign(sum(a_t * h_t(x))
```

## bagging

可采样出 T 个含 m 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合

Bagging 算法伪代码：

```
输入：训练集 D = {(x1, y1), (x2, y2), ... , (xm, my)};
     基学习算法 L;
     训练轮数 T;
过程：
for t = 1, 2, ... , T do
h_t = L(D, D_bs);
end for
输出：H(x)
```