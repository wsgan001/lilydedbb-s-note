# 决策树实战项目 —— 鸢尾花分类

## 特征选择

特征选择是建立决策树之前十分重要的一步

通常在选择特征时，会考虑到两种不同的指标，分别为：**信息增益和信息增益比**

熵（Entropy）是表示随机变量不确定性的度量。简单来讲，熵越大，随机变量的不确定性就越大。而特征 A 对于某一训练集 D 的信息增益 g(D, A) 定义为集合 D 的熵 H(D) 与特征 A 在给定条件下 D 的熵 H(D|A) 之差。

```
g(D, A) = H(B) - H(D|A)
```

## 生成算法

决策树的生成算法：

ID3 算法，这个算法的核心理论即源于上面提到的信息增益。ID3 算法通过递归的方式建立决策树。建立时，从根节点开始，对节点计算每个独立特征的信息增益，选择信息增益最大的特征作为节点特征。接下来，对该特征施加判断条件，建立子节点。然后针对子节点再此使用信息增益进行判断，直到所有特征的信息增益很小或者没有特征时结束，这样就逐步建立一颗完整的决策树。

除了从信息增益演化而来的 ID3 算法，还有一种常见的算法叫 C4.5。C4.5 算法使用了信息增益比来选择特征，这被看成是 ID3 算法的一种改进。

ID3 和 C4.5 算法简单高效，但是他俩均存在一个缺点，那就是用“完美去造就了另一个不完美”。这两个算法从信息增益和信息增益比开始，对整个训练集进行的分类，拟合出来的模型针对该训练集的确是非常完美的。但是，这种完美就使得整体模型的复杂度较高，而对其他数据集的预测能力就降低了，也就是常说的过拟合而使得模型的泛化能力变弱。

## 决策树修剪

CART 算法本身就包含了决策树的生成和修剪，并且可以同时被运用到分类树和回归树。这就是和 ID3 及 C4.5 之间的最大区别。

CART 算法在生成树的过程中，分类树采用了基尼指数（Gini Index）最小化原则，而回归树选择了平方损失函数最小化原则。基尼指数其实和前面提到的熵的概念是很相似的。简单概述区别的话，就是数值相近但不同，而基尼指数在运算过程中的速度会更快一些。

CART 算法也包含了树的修剪。CART 算法从完全生长的决策树底端剪去一些子树，使得模型更加简单。而修剪这些子树时，是每次去除一颗，逐步修剪直到根节点，从而形成一个子树序列。最后，对该子树序列进行交叉验证，再选出最优的子树作为最终决策树。

## 数据集

鸢尾花数据集：[Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris)

总共包含 150 行数据。每一行数据由 4 个特征值及一个目标值组成。其中 4 个特征值分别为：萼片长度、萼片宽度、花瓣长度、花瓣宽度。而目标值及为三种不同类别的鸢尾花，分别为：Iris Setosa，Iris Versicolour，Iris Virginica

载入数据集：

```python
# -*- coding: utf-8 -*-
from sklearn import datasets

iris = datasets.load_iris()
iris_feature = iris.data
iris_target = iris.target

print(iris_target)
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2]
```

这些数据是按照鸢尾花类别的顺序排列的。所以，如果将其直接划分为训练集和数据集的话，就会造成数据的分布不均。详细来讲，直接划分容易造成某种类型的花在训练集中一次都未出现，训练的模型就永远不可能预测出这种花来

scikit-learn 为提供了将这些数据大乱后再划分训练集和数据集的方法

```python
from sklearn.cross_validation import train_test_split
feature_train, feature_test, traget_train, target_test = train_test_split(
    iris_feature, iris_target, test_size=0.33, random_state=56
)
print(traget_train)
# [0 1 0 2 0 0 1 0 2 1 0 2 2 2 1 2 0 2 0 1 0 0 1 0 1 1 2 1 2 1 2 0 2 2 0 2 0
#  1 2 0 0 1 1 1 1 2 2 1 2 2 1 0 2 0 2 1 0 1 0 1 1 1 1 2 1 0 0 0 0 2 2 0 0 0
#  1 1 1 0 1 0 2 0 0 0 1 1 2 0 0 2 2 1 1 2 0 1 2 2 2 1]
```

## 模型训练及预测

从 scikit-learn 中导入决策树分类器。然后用 fit 方法和 predict 方法对模型进行训练和预测

可以通过 scikit-learn 中提供的评估计算方法查看预测结果的准确度

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

dt_model = DecisionTreeClassifier()
dt_model.fit(feature_train, target_train)
predict_result = dt_model.predict(feature_test)

print(target_test)     # [2 1 1 2 2 2 2 0 0 2 1 0 0 0 1 0 0 0 2 1 0 2 1 1 0 2 2 1 1 1 2 2 1 0 0 0 2 2 2 0 1 2 2 1 1 1 2 2 1 0]
print(predict_result)  # [2 1 2 2 2 2 2 0 0 2 1 0 0 0 2 0 0 0 2 1 0 2 2 1 0 2 2 1 1 1 2 2 2 0 0 0 2 2 2 0 2 2 2 2 1 1 2 2 1 0]
print(accuracy_score(predict_result, target_test))  # 0.88
```

------

[sklearn.tree.DecisionTreeClassifier](http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

[sklearn.tree.DecisionTreeClassifier源码](https://github.com/scikit-learn/scikit-learn/blob/67cbaef/sklearn/tree/tree.py)

**code：**

```python
# -*- coding: utf-8 -*-
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = datasets.load_iris()
iris_feature = iris.data
iris_target = iris.target

feature_train, feature_test, target_train, target_test = train_test_split(
    iris_feature, iris_target, test_size=0.33, random_state=56
)

dt_model = DecisionTreeClassifier()
dt_model.fit(feature_train, target_train)
predict_result = dt_model.predict(feature_test)

print(target_test)     # [2 1 1 2 2 2 2 0 0 2 1 0 0 0 1 0 0 0 2 1 0 2 1 1 0 2 2 1 1 1 2 2 1 0 0 0 2 2 2 0 1 2 2 1 1 1 2 2 1 0]
print(predict_result)  # [2 1 2 2 2 2 2 0 0 2 1 0 0 0 2 0 0 0 2 1 0 2 2 1 0 2 2 1 1 1 2 2 2 0 0 0 2 2 2 0 2 2 2 2 1 1 2 2 1 0]
print(accuracy_score(predict_result, target_test))  # 0.88
```
