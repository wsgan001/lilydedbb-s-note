# CS231n

## Image Classification

### K-Nearest Neighbors

#### Distance Metric

One interesting thing to point out between two metrics in particular, is that the L1 distance depends on your choice of coordinates system. So if you rotate the coordinate frame, that would actually change the L1 distance between the points. Whereas changing the coordinate frame in the L2 distance doesn't matter, it's the same thing no matter what your coordinate frame is.

If your input features, if the individual entries in your vector have some important meaning for your task, then maybe somehow L1 (Manhattan) distance might be a more natural fit. But if it's just a generic vector in some space and you don't know which of the different elements, you don't know what they actually mean, then maybe L2 (Euclidean) distance is slightly more natural.

#### drawbacks of k-Nearest Neighbors

If we expect the k-nearest neighbor classifier to work well, we kind of need our training examples to cover the space, quite densely.


## Loss Functions and Optimization

### Loss function

The loss function is the way you tell your algorithm what types of errors you care about and what types of errors it should trade off against.

#### SVM Loss function

```python
import numpy as np

def L_i_vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maximum(0, scores - scores[y] + 1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i
```

#### Regularization

In addition to the data loss, which will tell us classifier that it should fit the training data, we also typically add another term to the loss function, called a regularization term, which encourages the model to somehow pick a simpler W, where the concept of simple kind of depends on the task and the model.

```math
L(W) = \frac {1}{N} \sum_{i=1}^N L_i(f(x_i, W), y_i) + \lambda R(W)
```

- L1 Regularization:

```math
R(W) = \sum_k \sum_l |W_{k,l}|
```

**The general intuition behind L1 is that it generally prefers sparse solutions.**

- L2 Regularization:

```math
R(W) = \sum_k \sum_l W_{k,l} ^ 2
```

**The way of measuring complexity of L1 is maybe the number of non-zero entries. And L2, it thinks that things spread the W across all the value are less complex.**

- Elastic net (L1 + L2):

```math
R(W) = \sum_k \sum_l \beta W_{k,l} ^ 2  + \sum_k \sum_l |W_{k,l}|
```

- Max norm regularization
- Dropout
- Fancier

#### Softmax

scores = unnormalized log probabilities of the classes.

```math
P(Y = k | X = x_i) = \frac {e^{s_k}} {\sum_j {e^{s_j}}} , where s = f(x_i; W)
```

Want to maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class:

```math
L_i = -\logP(Y = y_i | X = x_i)
```

即：

```math
L_i = -\log(\frac {e^{s_k}} {\sum_j {e^{s_j}}})
```

#### Softmax vs. SVM

SVM, it will get this data point over the bar to be correctly classified and then just give up, it doesn't care about that data point any more. Whereas softmax will just always try to continually improve every single data point to get better and better.

### Optimize

- Numerical gradient: approximate, slow, easy to write
- Analytic gradient: exact, fast, error-prone

**In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.**

#### Stochastic Gradient Descent (SGD)

```math
L(W) = \frac {1}{N} \sum_{i=1}^N L_i(x_i, y_i, W) + \lambda R(W)
```

```math
\nabla_W L(W) = \frac {1}{N} \sum_{i=1}^N \nabla_WL_i(x_i, y_i, W) + \lambda \nabla_WR(W)
```

**Full sum expensive when N is large! Approximate sum using a minibatch of examples (32 / 64 / 128 common)**

#### Image Features

- Color Histogram
- Histogram of Oriented Gradients (HoG)
- Bag of Words


## Intorduction to Neural Networks

### sigmoid function

```math
\sigma(x) = \frac {1}{1 + e^{-x}}
```

```math
\frac {d\sigma(x)}{dx} = (1 - \sigma(x))\sigma(x)
```

### back propagation

#### Patterns in backward flow

- add gate: gradient distributor
- max gate: gradient router
- mul gate: gradient switcher

#### Jacobian matrix

Jacobian matrix: derivative of each element of z w.r.t. each element of x.

**Always check: The gradient with respect to a variable should have the same shape as the variable.**

A vectorized example:

```math
f(x, W) = {|| W \cdot x||}^2
```

```math
\nabla_W f = 2q \cdot x^T
```


```math
\nabla_x f = 2W^T \cdot q
```

### Neural Networks

pass


## Convolutional Neural Network

#### Output Size

```math
(N - F) / stride + 1
```

In practice: Common to zero pad the border

### The brain/neuron view of CONV Layer

An activation map is a sheet of neuron outputs:

1. Each is connected to a small region in the output
2. All of them share parameters

`n x n filter` -> `n x n receptive field for each neuron`

### Fully Connected Layer

Each neuron looks at the full input volume

### Pooling layer

> Looking at more recent neural network architectures, people begun to use strides more in order to do downsampling instead of just pooling.


## Training Neural Network

### Activation Function

#### Sigmoid

```math
\sigma(x) = \frac {1}{1 + e^{-x}}
```

- Squashes numbers to range [0, 1]
- Historically popular since they have nice interpretation as a saturating "firing rate" of a neuron

Problems：

1. Saturated neurons "kill" the gradients
2. Sigmoid outputs are not zero-centered

    > In general, we want input X to be zero meaned, so that we actually have positive and negative values, so we don't get into the problem of the gradient updates, that they'll be all moving in the same direction.

3. exp() is a bit compute expensive

#### tanh

- Squashes numbers to range [-1, 1]
- zero centered (nice)

Problems:

- still kills gradients when saturated :(

#### ReLU (Rectified Linear Unit)

```math
f(x) = max(0, x)
```

- Does not saturate (in +region)
- Very computationally efficient
- Converges much faster than
sigmoid/tanh in practice (e.g. 6x)
- Actually more biologically plausible than sigmoid

Problems:

1. Not zero-centered output
2. An annoyance: it kills the gradient in half of regime

**dead ReLU:**

> People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)

**Leaky ReLU**

```math
f(x) = max(0.01x, x)
```

- Does not saturate
- Computationally efficient
- Converges much faster than
sigmoid/tanh in practice! (e.g. 6x)
- will not “die”.

**PReLU (Parametric Rectifier)**

```math
f(x) = max(\alpha x, x)
```

**Exponential Linear Units (ELU)**

```math
f(x)= \begin{cases} x, \ if \  x > 0 \\ \alpha (e^x - 1), \ if \ x \leq 0 \end{cases}
```

- All benefits of ReLU
- Closer to zero mean outputs
- Negative saturation regime (compared with Leaky ReLU adds some robustness to noise)

#### Maxout “Neuron”

```math
max({w_1}^Tx + b_1, {w_2}^Tx + b_2)
```

- Does not have the basic form of dot product -> nonlinearity
- Generalizes ReLU and Leaky ReLU
- Linear Regime! Does not saturate! Does not die!

Problem:

- doubles the number of parameters/neuron :(

### Data Preprocessing

- zero mean
    ```python
    X -= np.mean(X, axis = 0)
    ```
- normalize
    ```python
    X /= np.std(X, axis = 0)
    ```
- PCA
- whitening

**Before normalization**: classification loss very sensitive to changes in weight matrix; hard to optimize

**After normalization**:

less sensitive to small changes in weights; easier to optimize

### Initialization

> When you initialize your weights at the start of training, if those weights are initialized to be too small, then the activation will vanish as you go through the network because as you multiply by these small numbers over and over again, they'll all sort of decay to zero. And everything will be zero, the learning won't happen.
>
> On the other hand, if you initialize your weights too big, then as you go through the network and multiply by your weight matrix over and over again, they will explode, there'll be no learning.

### Batch Normalization

To make each dimension unit gaussian:

```math
\hat{x}^{(k)} = \frac {x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
```

And then allow the network to squash the range if it wants to:

(recover the identity mapping)

```math
y^{(k)} = \gamma\hat{x}^{(k)} + \beta^{(k)}

\gamma^{(k)} = \sqrt{Var[x^{(k)}]}

\beta^{(k)} = E[x^{(k)}]
```

- Improves gradient flow through the network
- Allows higher learning rates
- Reduces the strong dependence
on initialization
- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe

### Train

#### Hyper parameter

**learning rate:**

- **loss not going down**: learning rate too low
- **loss exploding**: learning rate too high

> Rough range for learning rate we should be cross-validating is somewhere [1e-3 ... 1e-5]

### Optimization

#### Problems with SGD

1. What if loss changes quickly in one direction and slowly in another? What does gradient descent do?

    Very slow progress along shallow dimension, jitter along steep direction

    Loss function has high condition number: ratio of largest to smallest singular value of the Hessian matrix is large

2. local minima or saddle point

    **Saddle points much more common in high dimensions.**

3. The gradients come from mini batches so they can be noisy.

#### Solution: SGD + Momentum

**SGD**:

```math
x_{t+1} = x_t - \alpha \nabla f(x_t)
```

```python
while True:
    dx = compute_gradient(x)
    x += learning_rate * dx
```

**SGD + Momentum**:

```math
v_{t+1} = \rho v_t + \nabla f(x_t)

x_{t+1} = x_t - \alpha v_{t+1}
```

```python
vx = 0
while True:
    dx = compute_gradient(x)
    vx = rho * vx + dx
    x += learning_rate * vx
```

- Build up “velocity” as a running mean of gradients
- Rho gives “friction”; typically rho=0.9 or 0.99

**Nesterov Momentum**:

```math
v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)

x_{t+1} = x_t + v_{t+1}
```

Change of variances

```math
\tilde x_t = x_t + \rho v_t
```

and rearrange:

```math
v_{t+1} = \rho v_t - \alpha \nabla f(\tilde x_t)

\tilde x_{t+1} = \tilde x_t - \rho v_t + (1 + \rho)v_{t+1}

    = \tilde x_t + v_{t+1} + \rho(v_{t+1} - v_t)
```

> A very sharp minima actually could be a minima that overfits more. If you imagine that we double our training set, the whole optimization landscape would change and maybe that very sensitive minima would actually disappear if we were to collect more training data. We maybe want to land in very flat minima, because those very flat minima probably more robust as we change the training data. So the flat minima might generalize better to testing data.
>
> So in some sense, it's actually a feature not a bug that SGD Momentum actually skips over those very sharp minima.

**AdaGrad**:

```python
grad_squared = 0
while True:
    dx = compute_gradient(x)
    grad_squared += dx * dx
    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

Added element-wise scaling of the gradient based on the historical sum of squares in each dimension

> The idea with AdaGrad is that during the course of optimization, you're going to keep running estimate or a running sum of all the squared gradients.

- What happens with AdaGrad ?

    > The idea is that if we have two coordinates, one that always has a very high gradient, and one that always has a very small gradient, then as we add the sum of squares of small gradients, we're going to be dividing by a small number, so we will accelerate movement along that one dimension. Then on the other dimension where the gradients tend to be very large, then we'll be dividing by a large number, so we'll kind of slow down our progress along the wiggling dimension.

**RMSProp**:

```python
grad_squared = 0
while True:
    dx = compute_gradients(x)
    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx
    x -= learing_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

> Now with RMSProp, we still keep this estimate of squared gradients, but instead of just letting that squared estimate continually accumulate over training, instead we let that squared estimate actually decay. **So this ends up looking kind of like a momentum update, except we're having kind of momentum over the squared gradients rather than momentum over that actual gradients.**

**Adam**:

```python
first_time = 0
second_time = 0
while True:
    dx = compute_gradient(x)
    first_moment = beta1 * first_moment + (1 - beta1) * dx  # Momentum
    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx  # AdaGrad / RMSProp
    first_unbias = first_moment / (1 - beta1 ** t)  # bias correction
    second_unbias = second_moment / (1 - beta2 ** t)  # bias correction
    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)   # AdaGrad / RMSProp
```

**Bias correction for the fact that first and second moment estimates start at zero**

**Adam with beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4 is a great starting point for many models!**

> **Usually when you are kind of getting networks to work at the beginning, you want to pick a good learning rate with no learning rate decay from start. What you do for setting learning rate decay, is try with no decay and see what happens, then kind of eyeball the loss curve and see where you think you might need decay.**

#### First-Order Optimization

1. Use gradient form linear approximation
2. Step to minimize the approximation

#### Second-Order Optimization

1. Use gradient and Hessian to form quadratic approximation
2. Step to the minima of the approximation

second-order Taylor Expansion:

```math
J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^T \nabla_{\theta}J(\theta_0) + \frac 1 2 (\theta - \theta_0)^TH(\theta - \theta_0)
```

solving for the critical point we obtain the Newton parameter update:

```math
\theta_* = \theta_0 - H^{-1} \nabla_{\theta}J(\theta_0)
```

###  Model Ensembles

1. Train multiple independent models
2. 2. At test time average their results

** Model Ensembles: Tips and Tricks**

- Instead of training independent models, use multiple snapshots of a single model during training!

- Instead of using actual parameter vector, keep a moving average of the parameter vector and use that at test time (Polyak averaging)

```python
while True:
    data_batch = dataset.sample_data_batch()
    loss = network.forward(data_batch)
    dx = network.bachward()
    x += -learning_rate * dx
    x_test = 0.995 * x_test + 0.005 * x  # use for test set
```

### Regularization

#### Regularization: Add term to loss

```math
L = \frac 1 N \sum_{i = 1}^{N} \sum_{j \not=y_i}max(0, f(x_i; W)_j - f(x_i; W)_{y_j} + 1) + \lambda R(W)
```

L2 Regularization:

```math
R(W) = \sum_k \sum_l W_{k,l}^2
```

L1 Regularization:

```math
R(W) = \sum_k \sum_l |W_{k,l}|
```

Elastic net (L1 + L2):

```math
R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|
```

#### Regularization: Dropout

In each forward pass, randomly set some neurons to zero Probability of dropping is a hyper parameter.

**0.5 is common**

```python
p = 0.5

def train_step(X):
    H1 = np.maximum(0, np.dot(W1, X) + b1)
    U1 = np.random.rand(*H1.shape) < p  # dropout mask
    H1 *= U1  # drop
    H2 = np.maximum(0, np.dot(W2, H1) + b2)
    U2 = np.random.rand(*H2.shape) < p  # dropout mask
    H2 *= U2  # drop
    out = np.dot(W3, H2) + b3
```

**Another interpretation: Dropout is training a large ensemble of models (that share parameters). Each binary mask is one model.**

**Dropout: Test Time**:

Dropout makes our output random!

```math
y = fw(x, z)
```

```math
y = f(x) = E_z[f(x, z)] = \int p(z)f(x, z)dz
```

**At test time, multiply by dropout probability**

**At test time, all neurons are active always, we must scale the activations.**

```python
def predict(X):
    H1 = np.maximum(0, np.dot(W1, X) + b1) * p  # scale the activations
    H2 = np.maximun(0, np.dot(W2, H1) + b2) * p  # scale the activations
    out = np.dot(W3, H2) + b3
```

**More common: Inverted dropout**

```python
p = 0.5

def train_step(X):
    H1 = np.maximum(0, np.dot(W1, X) + b1)
    U1 = np.random.rand(*H1.shape) < p / p  # dropout mask
    H1 *= U1  # drop
    H2 = np.maximum(0, np.dot(W2, H1) + b2)
    U2 = np.random.rand(*H2.shape) < p / p  # dropout mask
    H2 *= U2  # drop
    out = np.dot(W3, H2) + b3

def predict(X):
    H1 = np.maximum(0, np.dot(W1, X) + b1)
    H2 = np.maximun(0, np.dot(W2, H1) + b2)
    out = np.dot(W3, H2) + b3
```

#### Regularization: Data augmentation

- Horizontal Flips
- Random Crops and Scales
- Color Jitter
    - Simple: Randomize contrast and brightness
    - More complex:
        1. Apply PCA to all [R, G, B] pixels in training set
        2. Sample a “color offset” along principal component directions
        3. Add offset to all pixels of a training image

### Regularization

- Dropout
- Batch Normalization
- Data Augmentation
- DropConnect
- Fractional Max Pooling
- Stochastic Depth

